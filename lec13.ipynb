{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "RNN"
      ],
      "metadata": {
        "id": "7K1LWMZgouX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. load dataset\n",
        "2. sentences - embeddings\n",
        "3. build RNN\n",
        "4. train\n",
        "5. predict"
      ],
      "metadata": {
        "id": "T9OuCMMcrllS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/100_Unique_QA_Dataset.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "S-P5lQVUotyW",
        "outputId": "73b870ec-ad1e-41e2-f13c-7c76d89159ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          question      answer\n",
              "0                   What is the capital of France?       Paris\n",
              "1                  What is the capital of Germany?      Berlin\n",
              "2               Who wrote 'To Kill a Mockingbird'?  Harper-Lee\n",
              "3  What is the largest planet in our solar system?     Jupiter\n",
              "4   What is the boiling point of water in Celsius?         100"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dee0c8a1-f5e2-4a03-beb2-e26df3495a95\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the capital of France?</td>\n",
              "      <td>Paris</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the capital of Germany?</td>\n",
              "      <td>Berlin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Who wrote 'To Kill a Mockingbird'?</td>\n",
              "      <td>Harper-Lee</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the largest planet in our solar system?</td>\n",
              "      <td>Jupiter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the boiling point of water in Celsius?</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dee0c8a1-f5e2-4a03-beb2-e26df3495a95')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dee0c8a1-f5e2-4a03-beb2-e26df3495a95 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dee0c8a1-f5e2-4a03-beb2-e26df3495a95');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 90,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 90,\n        \"samples\": [\n          \"What is the currency of China?\",\n          \"What is the capital of Australia?\",\n          \"Who discovered electricity?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 85,\n        \"samples\": [\n          \"ChristopherColumbus\",\n          \"Paris\",\n          \"Christmas\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize\n",
        "def tokenize(text):\n",
        "  text = text.lower()\n",
        "  text = text.replace('?' , '')\n",
        "  text = text.replace(\"'\" , \"\")\n",
        "  return text.split()"
      ],
      "metadata": {
        "id": "Xdm6igBNr3bL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize(\"'hello' world?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htuO_ZnMsE2S",
        "outputId": "4ff331bd-b932-4456-da95-65b60abf0ca1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'world']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab\n",
        "vocab = {'<UNK>' : 0}"
      ],
      "metadata": {
        "id": "E5i3BVPMr61q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(row):\n",
        "  print(row['question'] , row['answer'])\n",
        "  tokenized_question = tokenize(row['question'])\n",
        "  tokenized_answer = tokenize(row['answer'])\n",
        "\n",
        "  merged_tokens = tokenized_question + tokenized_answer\n",
        "\n",
        "  for token in merged_tokens:\n",
        "    if token not in vocab:\n",
        "      vocab[token] = len(vocab)"
      ],
      "metadata": {
        "id": "0I7pzIRqsm2m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.apply(build_vocab , axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kIHXR-fVst5v",
        "outputId": "48a54bd3-0763-4e51-b20d-0ea27542df95"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the capital of France? Paris\n",
            "What is the capital of Germany? Berlin\n",
            "Who wrote 'To Kill a Mockingbird'? Harper-Lee\n",
            "What is the largest planet in our solar system? Jupiter\n",
            "What is the boiling point of water in Celsius? 100\n",
            "Who painted the Mona Lisa? Leonardo-da-Vinci\n",
            "What is the square root of 64? 8\n",
            "What is the chemical symbol for gold? Au\n",
            "Which year did World War II end? 1945\n",
            "What is the longest river in the world? Nile\n",
            "What is the capital of Japan? Tokyo\n",
            "Who developed the theory of relativity? Albert-Einstein\n",
            "What is the freezing point of water in Fahrenheit? 32\n",
            "Which planet is known as the Red Planet? Mars\n",
            "Who is the author of '1984'? George-Orwell\n",
            "What is the currency of the United Kingdom? Pound\n",
            "What is the capital of India? Delhi\n",
            "Who discovered gravity? Newton\n",
            "How many continents are there on Earth? 7\n",
            "Which gas do plants use for photosynthesis? CO2\n",
            "What is the smallest prime number? 2\n",
            "Who invented the telephone? Alexander-Graham-Bell\n",
            "What is the capital of Australia? Canberra\n",
            "Which ocean is the largest? Pacific-Ocean\n",
            "What is the speed of light in vacuum? 299,792,458m/s\n",
            "Which language is spoken in Brazil? Portuguese\n",
            "Who discovered penicillin? Alexander-Fleming\n",
            "What is the capital of Canada? Ottawa\n",
            "What is the largest mammal on Earth? Whale\n",
            "Which element has the atomic number 1? Hydrogen\n",
            "What is the tallest mountain in the world? Everest\n",
            "Which city is known as the Big Apple? NewYork\n",
            "How many planets are in the Solar System? 8\n",
            "Who painted 'Starry Night'? vangogh\n",
            "What is the chemical formula of water? H2O\n",
            "What is the capital of Italy? Rome\n",
            "Which country is famous for sushi? Japan\n",
            "Who was the first person to step on the Moon? Armstrong\n",
            "What is the main ingredient in guacamole? Avocado\n",
            "How many sides does a hexagon have? 6\n",
            "What is the currency of China? Yuan\n",
            "Who wrote 'Pride and Prejudice'? Jane-Austen\n",
            "What is the chemical symbol for iron? Fe\n",
            "What is the hardest natural substance on Earth? Diamond\n",
            "Which continent is the largest by area? Asia\n",
            "Who was the first President of the United States? George-Washington\n",
            "Which bird is known for its ability to mimic sounds? Parrot\n",
            "What is the longest-running animated TV show? Simpsons\n",
            "What is the smallest country in the world? VaticanCity\n",
            "Which planet has the most moons? Saturn\n",
            "Who wrote 'Romeo and Juliet'? Shakespeare\n",
            "What is the main gas in Earth's atmosphere? Nitrogen\n",
            "How many bones are in the adult human body? 206\n",
            "Which metal is a liquid at room temperature? Mercury\n",
            "What is the capital of Russia? Moscow\n",
            "Who discovered electricity? Benjamin-Franklin\n",
            "Which is the second-largest country by land area? Canada\n",
            "What is the color of a ripe banana? Yellow\n",
            "Which month has 28 days in a common year? February\n",
            "What is the study of living organisms called? Biology\n",
            "Which country is home to the Great Wall? China\n",
            "What do bees collect from flowers? Nectar\n",
            "What is the opposite of 'day'? Night\n",
            "What is the capital of South Korea? Seoul\n",
            "Who invented the light bulb? Edison\n",
            "Which gas do humans breathe in for survival? Oxygen\n",
            "What is the square root of 144? 12\n",
            "Which country has the pyramids of Giza? Egypt\n",
            "Which sea creature has eight arms? Octopus\n",
            "Which holiday is celebrated on December 25? Christmas\n",
            "What is the currency of Japan? Yen\n",
            "How many legs does a spider have? 8\n",
            "Which sport uses a net, ball, and hoop? Basketball\n",
            "Which country is famous for its kangaroos? Australia\n",
            "Who was the first female Prime Minister of the UK? MargaretThatcher\n",
            "Which is the fastest land animal? Cheetah\n",
            "What is the first element on the periodic table? Hydrogen\n",
            "What is the capital of Spain? Madrid\n",
            "Which planet is the closest to the Sun? Mercury\n",
            "Who is known as the father of computers? CharlesBabbage\n",
            "What is the capital of Mexico? MexicoCity\n",
            "How many colors are in a rainbow? 7\n",
            "Which musical instrument has black and white keys? Piano\n",
            "Who discovered the Americas in 1492? ChristopherColumbus\n",
            "Which Disney character has a long nose and grows it when lying? Pinocchio\n",
            "Who directed the movie 'Titanic'? JamesCameron\n",
            "Which superhero is also known as the Dark Knight? Batman\n",
            "What is the capital of Brazil? Brasilia\n",
            "Which fruit is known as the king of fruits? Mango\n",
            "Which country is known for the Eiffel Tower? France\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     None\n",
              "1     None\n",
              "2     None\n",
              "3     None\n",
              "4     None\n",
              "      ... \n",
              "85    None\n",
              "86    None\n",
              "87    None\n",
              "88    None\n",
              "89    None\n",
              "Length: 90, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>90 rows Ã— 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA0gxY3DtJvD",
        "outputId": "7fba17e3-34cf-4587-cff8-b30795cc9f88"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<UNK>': 0,\n",
              " 'what': 1,\n",
              " 'is': 2,\n",
              " 'the': 3,\n",
              " 'capital': 4,\n",
              " 'of': 5,\n",
              " 'france': 6,\n",
              " 'paris': 7,\n",
              " 'germany': 8,\n",
              " 'berlin': 9,\n",
              " 'who': 10,\n",
              " 'wrote': 11,\n",
              " 'to': 12,\n",
              " 'kill': 13,\n",
              " 'a': 14,\n",
              " 'mockingbird': 15,\n",
              " 'harper-lee': 16,\n",
              " 'largest': 17,\n",
              " 'planet': 18,\n",
              " 'in': 19,\n",
              " 'our': 20,\n",
              " 'solar': 21,\n",
              " 'system': 22,\n",
              " 'jupiter': 23,\n",
              " 'boiling': 24,\n",
              " 'point': 25,\n",
              " 'water': 26,\n",
              " 'celsius': 27,\n",
              " '100': 28,\n",
              " 'painted': 29,\n",
              " 'mona': 30,\n",
              " 'lisa': 31,\n",
              " 'leonardo-da-vinci': 32,\n",
              " 'square': 33,\n",
              " 'root': 34,\n",
              " '64': 35,\n",
              " '8': 36,\n",
              " 'chemical': 37,\n",
              " 'symbol': 38,\n",
              " 'for': 39,\n",
              " 'gold': 40,\n",
              " 'au': 41,\n",
              " 'which': 42,\n",
              " 'year': 43,\n",
              " 'did': 44,\n",
              " 'world': 45,\n",
              " 'war': 46,\n",
              " 'ii': 47,\n",
              " 'end': 48,\n",
              " '1945': 49,\n",
              " 'longest': 50,\n",
              " 'river': 51,\n",
              " 'nile': 52,\n",
              " 'japan': 53,\n",
              " 'tokyo': 54,\n",
              " 'developed': 55,\n",
              " 'theory': 56,\n",
              " 'relativity': 57,\n",
              " 'albert-einstein': 58,\n",
              " 'freezing': 59,\n",
              " 'fahrenheit': 60,\n",
              " '32': 61,\n",
              " 'known': 62,\n",
              " 'as': 63,\n",
              " 'red': 64,\n",
              " 'mars': 65,\n",
              " 'author': 66,\n",
              " '1984': 67,\n",
              " 'george-orwell': 68,\n",
              " 'currency': 69,\n",
              " 'united': 70,\n",
              " 'kingdom': 71,\n",
              " 'pound': 72,\n",
              " 'india': 73,\n",
              " 'delhi': 74,\n",
              " 'discovered': 75,\n",
              " 'gravity': 76,\n",
              " 'newton': 77,\n",
              " 'how': 78,\n",
              " 'many': 79,\n",
              " 'continents': 80,\n",
              " 'are': 81,\n",
              " 'there': 82,\n",
              " 'on': 83,\n",
              " 'earth': 84,\n",
              " '7': 85,\n",
              " 'gas': 86,\n",
              " 'do': 87,\n",
              " 'plants': 88,\n",
              " 'use': 89,\n",
              " 'photosynthesis': 90,\n",
              " 'co2': 91,\n",
              " 'smallest': 92,\n",
              " 'prime': 93,\n",
              " 'number': 94,\n",
              " '2': 95,\n",
              " 'invented': 96,\n",
              " 'telephone': 97,\n",
              " 'alexander-graham-bell': 98,\n",
              " 'australia': 99,\n",
              " 'canberra': 100,\n",
              " 'ocean': 101,\n",
              " 'pacific-ocean': 102,\n",
              " 'speed': 103,\n",
              " 'light': 104,\n",
              " 'vacuum': 105,\n",
              " '299,792,458m/s': 106,\n",
              " 'language': 107,\n",
              " 'spoken': 108,\n",
              " 'brazil': 109,\n",
              " 'portuguese': 110,\n",
              " 'penicillin': 111,\n",
              " 'alexander-fleming': 112,\n",
              " 'canada': 113,\n",
              " 'ottawa': 114,\n",
              " 'mammal': 115,\n",
              " 'whale': 116,\n",
              " 'element': 117,\n",
              " 'has': 118,\n",
              " 'atomic': 119,\n",
              " '1': 120,\n",
              " 'hydrogen': 121,\n",
              " 'tallest': 122,\n",
              " 'mountain': 123,\n",
              " 'everest': 124,\n",
              " 'city': 125,\n",
              " 'big': 126,\n",
              " 'apple': 127,\n",
              " 'newyork': 128,\n",
              " 'planets': 129,\n",
              " 'starry': 130,\n",
              " 'night': 131,\n",
              " 'vangogh': 132,\n",
              " 'formula': 133,\n",
              " 'h2o': 134,\n",
              " 'italy': 135,\n",
              " 'rome': 136,\n",
              " 'country': 137,\n",
              " 'famous': 138,\n",
              " 'sushi': 139,\n",
              " 'was': 140,\n",
              " 'first': 141,\n",
              " 'person': 142,\n",
              " 'step': 143,\n",
              " 'moon': 144,\n",
              " 'armstrong': 145,\n",
              " 'main': 146,\n",
              " 'ingredient': 147,\n",
              " 'guacamole': 148,\n",
              " 'avocado': 149,\n",
              " 'sides': 150,\n",
              " 'does': 151,\n",
              " 'hexagon': 152,\n",
              " 'have': 153,\n",
              " '6': 154,\n",
              " 'china': 155,\n",
              " 'yuan': 156,\n",
              " 'pride': 157,\n",
              " 'and': 158,\n",
              " 'prejudice': 159,\n",
              " 'jane-austen': 160,\n",
              " 'iron': 161,\n",
              " 'fe': 162,\n",
              " 'hardest': 163,\n",
              " 'natural': 164,\n",
              " 'substance': 165,\n",
              " 'diamond': 166,\n",
              " 'continent': 167,\n",
              " 'by': 168,\n",
              " 'area': 169,\n",
              " 'asia': 170,\n",
              " 'president': 171,\n",
              " 'states': 172,\n",
              " 'george-washington': 173,\n",
              " 'bird': 174,\n",
              " 'its': 175,\n",
              " 'ability': 176,\n",
              " 'mimic': 177,\n",
              " 'sounds': 178,\n",
              " 'parrot': 179,\n",
              " 'longest-running': 180,\n",
              " 'animated': 181,\n",
              " 'tv': 182,\n",
              " 'show': 183,\n",
              " 'simpsons': 184,\n",
              " 'vaticancity': 185,\n",
              " 'most': 186,\n",
              " 'moons': 187,\n",
              " 'saturn': 188,\n",
              " 'romeo': 189,\n",
              " 'juliet': 190,\n",
              " 'shakespeare': 191,\n",
              " 'earths': 192,\n",
              " 'atmosphere': 193,\n",
              " 'nitrogen': 194,\n",
              " 'bones': 195,\n",
              " 'adult': 196,\n",
              " 'human': 197,\n",
              " 'body': 198,\n",
              " '206': 199,\n",
              " 'metal': 200,\n",
              " 'liquid': 201,\n",
              " 'at': 202,\n",
              " 'room': 203,\n",
              " 'temperature': 204,\n",
              " 'mercury': 205,\n",
              " 'russia': 206,\n",
              " 'moscow': 207,\n",
              " 'electricity': 208,\n",
              " 'benjamin-franklin': 209,\n",
              " 'second-largest': 210,\n",
              " 'land': 211,\n",
              " 'color': 212,\n",
              " 'ripe': 213,\n",
              " 'banana': 214,\n",
              " 'yellow': 215,\n",
              " 'month': 216,\n",
              " '28': 217,\n",
              " 'days': 218,\n",
              " 'common': 219,\n",
              " 'february': 220,\n",
              " 'study': 221,\n",
              " 'living': 222,\n",
              " 'organisms': 223,\n",
              " 'called': 224,\n",
              " 'biology': 225,\n",
              " 'home': 226,\n",
              " 'great': 227,\n",
              " 'wall': 228,\n",
              " 'bees': 229,\n",
              " 'collect': 230,\n",
              " 'from': 231,\n",
              " 'flowers': 232,\n",
              " 'nectar': 233,\n",
              " 'opposite': 234,\n",
              " 'day': 235,\n",
              " 'south': 236,\n",
              " 'korea': 237,\n",
              " 'seoul': 238,\n",
              " 'bulb': 239,\n",
              " 'edison': 240,\n",
              " 'humans': 241,\n",
              " 'breathe': 242,\n",
              " 'survival': 243,\n",
              " 'oxygen': 244,\n",
              " '144': 245,\n",
              " '12': 246,\n",
              " 'pyramids': 247,\n",
              " 'giza': 248,\n",
              " 'egypt': 249,\n",
              " 'sea': 250,\n",
              " 'creature': 251,\n",
              " 'eight': 252,\n",
              " 'arms': 253,\n",
              " 'octopus': 254,\n",
              " 'holiday': 255,\n",
              " 'celebrated': 256,\n",
              " 'december': 257,\n",
              " '25': 258,\n",
              " 'christmas': 259,\n",
              " 'yen': 260,\n",
              " 'legs': 261,\n",
              " 'spider': 262,\n",
              " 'sport': 263,\n",
              " 'uses': 264,\n",
              " 'net,': 265,\n",
              " 'ball,': 266,\n",
              " 'hoop': 267,\n",
              " 'basketball': 268,\n",
              " 'kangaroos': 269,\n",
              " 'female': 270,\n",
              " 'minister': 271,\n",
              " 'uk': 272,\n",
              " 'margaretthatcher': 273,\n",
              " 'fastest': 274,\n",
              " 'animal': 275,\n",
              " 'cheetah': 276,\n",
              " 'periodic': 277,\n",
              " 'table': 278,\n",
              " 'spain': 279,\n",
              " 'madrid': 280,\n",
              " 'closest': 281,\n",
              " 'sun': 282,\n",
              " 'father': 283,\n",
              " 'computers': 284,\n",
              " 'charlesbabbage': 285,\n",
              " 'mexico': 286,\n",
              " 'mexicocity': 287,\n",
              " 'colors': 288,\n",
              " 'rainbow': 289,\n",
              " 'musical': 290,\n",
              " 'instrument': 291,\n",
              " 'black': 292,\n",
              " 'white': 293,\n",
              " 'keys': 294,\n",
              " 'piano': 295,\n",
              " 'americas': 296,\n",
              " '1492': 297,\n",
              " 'christophercolumbus': 298,\n",
              " 'disney': 299,\n",
              " 'character': 300,\n",
              " 'long': 301,\n",
              " 'nose': 302,\n",
              " 'grows': 303,\n",
              " 'it': 304,\n",
              " 'when': 305,\n",
              " 'lying': 306,\n",
              " 'pinocchio': 307,\n",
              " 'directed': 308,\n",
              " 'movie': 309,\n",
              " 'titanic': 310,\n",
              " 'jamescameron': 311,\n",
              " 'superhero': 312,\n",
              " 'also': 313,\n",
              " 'dark': 314,\n",
              " 'knight': 315,\n",
              " 'batman': 316,\n",
              " 'brasilia': 317,\n",
              " 'fruit': 318,\n",
              " 'king': 319,\n",
              " 'fruits': 320,\n",
              " 'mango': 321,\n",
              " 'eiffel': 322,\n",
              " 'tower': 323}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert words to numerical indices\n",
        "def text_to_indices(text , vocab):\n",
        "  indexed_text = []\n",
        "  for token in tokenize(text):\n",
        "    if token in vocab:\n",
        "      indexed_text.append(vocab[token])\n",
        "    else:\n",
        "      indexed_text.append(vocab['<UNK>'])\n",
        "\n",
        "  return indexed_text"
      ],
      "metadata": {
        "id": "XeW_L7nMr7tZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_indices('what is jayant' , vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kns9-CNZ-PVg",
        "outputId": "106bdc71-1f83-4a58-c532-a75baebc806e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "m6Q3aGzp-RAX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QADataset(Dataset):\n",
        "  def __init__(self , df , vocab):\n",
        "    self.df = df\n",
        "    self.vocab = vocab\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.df.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    numerical_question = text_to_indices(self.df.iloc[index]['question'] , self.vocab)\n",
        "    numerical_answer = text_to_indices(self.df.iloc[index]['answer'] , self.vocab)\n",
        "\n",
        "    return torch.tensor(numerical_question) , torch.tensor(numerical_answer)"
      ],
      "metadata": {
        "id": "nN3XS7ly-dLg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = QADataset(df , vocab)"
      ],
      "metadata": {
        "id": "jz9faKer-8cR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset , batch_size=1 , shuffle=True)"
      ],
      "metadata": {
        "id": "dKXWqsns_CJ3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for question , answer in dataloader:\n",
        "  print(question)\n",
        "  print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKP6-RBx_ENn",
        "outputId": "1f05c8e2-bbf0-41ef-b94c-10f9d3f99406"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 10,   2,  62,  63,   3, 283,   5, 284]])\n",
            "tensor([[285]])\n",
            "tensor([[ 1,  2,  3, 33, 34,  5, 35]])\n",
            "tensor([[36]])\n",
            "tensor([[ 10,  11, 157, 158, 159]])\n",
            "tensor([[160]])\n",
            "tensor([[ 1,  2,  3, 59, 25,  5, 26, 19, 60]])\n",
            "tensor([[61]])\n",
            "tensor([[  1,   2,   3,   4,   5, 236, 237]])\n",
            "tensor([[238]])\n",
            "tensor([[ 42, 312,   2, 313,  62,  63,   3, 314, 315]])\n",
            "tensor([[316]])\n",
            "tensor([[  1,   2,   3,   4,   5, 206]])\n",
            "tensor([[207]])\n",
            "tensor([[  1,   2,   3, 221,   5, 222, 223, 224]])\n",
            "tensor([[225]])\n",
            "tensor([[ 1,  2,  3, 50, 51, 19,  3, 45]])\n",
            "tensor([[52]])\n",
            "tensor([[ 10, 140,   3, 141, 171,   5,   3,  70, 172]])\n",
            "tensor([[173]])\n",
            "tensor([[ 1,  2,  3, 92, 93, 94]])\n",
            "tensor([[95]])\n",
            "tensor([[78, 79, 80, 81, 82, 83, 84]])\n",
            "tensor([[85]])\n",
            "tensor([[ 42, 318,   2,  62,  63,   3, 319,   5, 320]])\n",
            "tensor([[321]])\n",
            "tensor([[ 42,   2,   3, 274, 211, 275]])\n",
            "tensor([[276]])\n",
            "tensor([[ 42, 117, 118,   3, 119,  94, 120]])\n",
            "tensor([[121]])\n",
            "tensor([[ 42, 137,   2, 138,  39, 175, 269]])\n",
            "tensor([[99]])\n",
            "tensor([[  1,   2,   3,   4,   5, 279]])\n",
            "tensor([[280]])\n",
            "tensor([[ 10, 140,   3, 141, 142,  12, 143,  83,   3, 144]])\n",
            "tensor([[145]])\n",
            "tensor([[  1,   2,   3, 141, 117,  83,   3, 277, 278]])\n",
            "tensor([[121]])\n",
            "tensor([[ 42, 137,   2,  62,  39,   3, 322, 323]])\n",
            "tensor([[6]])\n",
            "tensor([[  1,   2,   3,  69,   5, 155]])\n",
            "tensor([[156]])\n",
            "tensor([[ 42, 125,   2,  62,  63,   3, 126, 127]])\n",
            "tensor([[128]])\n",
            "tensor([[ 42, 137,   2, 226,  12,   3, 227, 228]])\n",
            "tensor([[155]])\n",
            "tensor([[10, 55,  3, 56,  5, 57]])\n",
            "tensor([[58]])\n",
            "tensor([[ 78,  79, 261, 151,  14, 262, 153]])\n",
            "tensor([[36]])\n",
            "tensor([[ 42,   2,   3, 210, 137, 168, 211, 169]])\n",
            "tensor([[113]])\n",
            "tensor([[ 1,  2,  3,  4,  5, 99]])\n",
            "tensor([[100]])\n",
            "tensor([[ 42, 200,   2,  14, 201, 202, 203, 204]])\n",
            "tensor([[205]])\n",
            "tensor([[  1,   2,   3, 103,   5, 104,  19, 105]])\n",
            "tensor([[106]])\n",
            "tensor([[10, 96,  3, 97]])\n",
            "tensor([[98]])\n",
            "tensor([[ 42, 263, 264,  14, 265, 266, 158, 267]])\n",
            "tensor([[268]])\n",
            "tensor([[  1,   2,   3,  92, 137,  19,   3,  45]])\n",
            "tensor([[185]])\n",
            "tensor([[ 10, 140,   3, 141, 270,  93, 271,   5,   3, 272]])\n",
            "tensor([[273]])\n",
            "tensor([[ 42, 101,   2,   3,  17]])\n",
            "tensor([[102]])\n",
            "tensor([[ 42, 137,   2, 138,  39, 139]])\n",
            "tensor([[53]])\n",
            "tensor([[  1,   2,   3,   4,   5, 109]])\n",
            "tensor([[317]])\n",
            "tensor([[  1,   2,   3, 146,  86,  19, 192, 193]])\n",
            "tensor([[194]])\n",
            "tensor([[  1,   2,   3,   4,   5, 286]])\n",
            "tensor([[287]])\n",
            "tensor([[ 1,  2,  3,  4,  5, 53]])\n",
            "tensor([[54]])\n",
            "tensor([[ 10,  75,   3, 296,  19, 297]])\n",
            "tensor([[298]])\n",
            "tensor([[  1,   2,   3,  37, 133,   5,  26]])\n",
            "tensor([[134]])\n",
            "tensor([[ 78,  79, 195,  81,  19,   3, 196, 197, 198]])\n",
            "tensor([[199]])\n",
            "tensor([[ 10, 308,   3, 309, 310]])\n",
            "tensor([[311]])\n",
            "tensor([[  1,   2,   3,   4,   5, 135]])\n",
            "tensor([[136]])\n",
            "tensor([[42, 18,  2, 62, 63,  3, 64, 18]])\n",
            "tensor([[65]])\n",
            "tensor([[ 42, 250, 251, 118, 252, 253]])\n",
            "tensor([[254]])\n",
            "tensor([[42, 86, 87, 88, 89, 39, 90]])\n",
            "tensor([[91]])\n",
            "tensor([[ 42, 107,   2, 108,  19, 109]])\n",
            "tensor([[110]])\n",
            "tensor([[ 42, 167,   2,   3,  17, 168, 169]])\n",
            "tensor([[170]])\n",
            "tensor([[ 10,  75, 208]])\n",
            "tensor([[209]])\n",
            "tensor([[ 78,  79, 129,  81,  19,   3,  21,  22]])\n",
            "tensor([[36]])\n",
            "tensor([[ 10,  75, 111]])\n",
            "tensor([[112]])\n",
            "tensor([[ 42, 255,   2, 256,  83, 257, 258]])\n",
            "tensor([[259]])\n",
            "tensor([[10, 75, 76]])\n",
            "tensor([[77]])\n",
            "tensor([[  1,   2,   3,  17, 115,  83,  84]])\n",
            "tensor([[116]])\n",
            "tensor([[  1,   2,   3, 180, 181, 182, 183]])\n",
            "tensor([[184]])\n",
            "tensor([[  1,   2,   3, 212,   5,  14, 213, 214]])\n",
            "tensor([[215]])\n",
            "tensor([[  1,   2,   3, 122, 123,  19,   3,  45]])\n",
            "tensor([[124]])\n",
            "tensor([[ 10,  96,   3, 104, 239]])\n",
            "tensor([[240]])\n",
            "tensor([[ 42,  18,   2,   3, 281,  12,   3, 282]])\n",
            "tensor([[205]])\n",
            "tensor([[ 10,  29, 130, 131]])\n",
            "tensor([[132]])\n",
            "tensor([[ 1,  2,  3, 17, 18, 19, 20, 21, 22]])\n",
            "tensor([[23]])\n",
            "tensor([[ 42, 290, 291, 118, 292, 158, 293, 294]])\n",
            "tensor([[295]])\n",
            "tensor([[  1,  87, 229, 230, 231, 232]])\n",
            "tensor([[233]])\n",
            "tensor([[1, 2, 3, 4, 5, 6]])\n",
            "tensor([[7]])\n",
            "tensor([[ 42,  18, 118,   3, 186, 187]])\n",
            "tensor([[188]])\n",
            "tensor([[ 42,  86,  87, 241, 242,  19,  39, 243]])\n",
            "tensor([[244]])\n",
            "tensor([[10,  2,  3, 66,  5, 67]])\n",
            "tensor([[68]])\n",
            "tensor([[ 78,  79, 150, 151,  14, 152, 153]])\n",
            "tensor([[154]])\n",
            "tensor([[ 42, 174,   2,  62,  39, 175, 176,  12, 177, 178]])\n",
            "tensor([[179]])\n",
            "tensor([[  1,   2,   3,   4,   5, 113]])\n",
            "tensor([[114]])\n",
            "tensor([[42, 43, 44, 45, 46, 47, 48]])\n",
            "tensor([[49]])\n",
            "tensor([[ 1,  2,  3, 24, 25,  5, 26, 19, 27]])\n",
            "tensor([[28]])\n",
            "tensor([[ 1,  2,  3, 69,  5, 53]])\n",
            "tensor([[260]])\n",
            "tensor([[ 42, 299, 300, 118,  14, 301, 302, 158, 303, 304, 305, 306]])\n",
            "tensor([[307]])\n",
            "tensor([[ 42, 216, 118, 217, 218,  19,  14, 219,  43]])\n",
            "tensor([[220]])\n",
            "tensor([[1, 2, 3, 4, 5, 8]])\n",
            "tensor([[9]])\n",
            "tensor([[10, 11, 12, 13, 14, 15]])\n",
            "tensor([[16]])\n",
            "tensor([[ 10,  11, 189, 158, 190]])\n",
            "tensor([[191]])\n",
            "tensor([[ 42, 137, 118,   3, 247,   5, 248]])\n",
            "tensor([[249]])\n",
            "tensor([[10, 29,  3, 30, 31]])\n",
            "tensor([[32]])\n",
            "tensor([[  1,   2,   3, 146, 147,  19, 148]])\n",
            "tensor([[149]])\n",
            "tensor([[ 1,  2,  3, 69,  5,  3, 70, 71]])\n",
            "tensor([[72]])\n",
            "tensor([[  1,   2,   3,  37,  38,  39, 161]])\n",
            "tensor([[162]])\n",
            "tensor([[  1,   2,   3,  33,  34,   5, 245]])\n",
            "tensor([[246]])\n",
            "tensor([[  1,   2,   3, 234,   5, 235]])\n",
            "tensor([[131]])\n",
            "tensor([[  1,   2,   3, 163, 164, 165,  83,  84]])\n",
            "tensor([[166]])\n",
            "tensor([[ 1,  2,  3, 37, 38, 39, 40]])\n",
            "tensor([[41]])\n",
            "tensor([[ 78,  79, 288,  81,  19,  14, 289]])\n",
            "tensor([[85]])\n",
            "tensor([[ 1,  2,  3,  4,  5, 73]])\n",
            "tensor([[74]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN architechture\n",
        "\n",
        "\n",
        "1.   input layer - 50 neurons\n",
        "2.   hidden layer - 64 neurons\n",
        "3.   output layer - 324 neurons\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EqJ_GlW0_mNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n"
      ],
      "metadata": {
        "id": "94CjyP-6_hwX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sequential cannot be used here as it expects 1 output but here rnn gives multiple outputs\n"
      ],
      "metadata": {
        "id": "AjuU0TJxB5tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "  def __init__ (self , vocab_size):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size , embedding_dim=50)\n",
        "    self.rnn = nn.RNN(50,64 , batch_first=True)\n",
        "    self.fc = nn.Linear(64,vocab_size)\n",
        "    # fc = fully connected\n",
        "\n",
        "  def forward(self , question):\n",
        "    embedded_question = self.embedding(question)\n",
        "    hidden , final = self.rnn(embedded_question)\n",
        "    output = self.fc(final.squeeze(0))\n",
        "    return output"
      ],
      "metadata": {
        "id": "XWCiS-dlAOog"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = nn.Embedding(324, embedding_dim=50)\n",
        "y = nn.RNN(50, 64, batch_first=True)\n",
        "z = nn.Linear(64, 324)\n",
        "\n",
        "a = dataset[0][0].reshape(1,6)\n",
        "print(\"shape of a:\", a.shape)\n",
        "b = x(a)\n",
        "print(\"shape of b:\", b.shape)\n",
        "c, d = y(b)\n",
        "print(\"shape of c:\", c.shape)\n",
        "print(\"shape of d:\", d.shape)\n",
        "\n",
        "e = z(d.squeeze(0))\n",
        "\n",
        "print(\"shape of e:\", e.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojWlsn-LDSUZ",
        "outputId": "33cfeed7-1481-4d7d-b799-34f9f9b5262a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of a: torch.Size([1, 6])\n",
            "shape of b: torch.Size([1, 6, 50])\n",
            "shape of c: torch.Size([1, 6, 64])\n",
            "shape of d: torch.Size([1, 1, 64])\n",
            "shape of e: torch.Size([1, 324])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "epochs = 20\n",
        "\n",
        "model = SimpleRNN(len(vocab))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters() , lr=learning_rate)"
      ],
      "metadata": {
        "id": "MH8ZYSz0Bo1V"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  total_loss = 0\n",
        "  for question , answer in dataloader:\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward pass\n",
        "    output = model(question)\n",
        "\n",
        "    # loss - > o/p shape (1,324) -> (1)\n",
        "    loss = criterion(output , answer[0])\n",
        "\n",
        "    # gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch + 1} , Loss : {total_loss:4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksdtrVCyCjbI",
        "outputId": "16120ce5-de64-4f9b-ff6b-242ff49bcde1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 , Loss : 528.702246\n",
            "Epoch 2 , Loss : 463.322827\n",
            "Epoch 3 , Loss : 385.712003\n",
            "Epoch 4 , Loss : 320.783295\n",
            "Epoch 5 , Loss : 268.083035\n",
            "Epoch 6 , Loss : 218.253635\n",
            "Epoch 7 , Loss : 173.498209\n",
            "Epoch 8 , Loss : 134.494075\n",
            "Epoch 9 , Loss : 101.999968\n",
            "Epoch 10 , Loss : 77.535278\n",
            "Epoch 11 , Loss : 58.965096\n",
            "Epoch 12 , Loss : 45.747361\n",
            "Epoch 13 , Loss : 35.953177\n",
            "Epoch 14 , Loss : 28.881290\n",
            "Epoch 15 , Loss : 23.476401\n",
            "Epoch 16 , Loss : 19.469569\n",
            "Epoch 17 , Loss : 16.303102\n",
            "Epoch 18 , Loss : 13.881577\n",
            "Epoch 19 , Loss : 11.884869\n",
            "Epoch 20 , Loss : 10.259787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, question, threshold=0.5):\n",
        "\n",
        "  # convert question to numbers\n",
        "  numerical_question = text_to_indices(question, vocab)\n",
        "\n",
        "  # tensor\n",
        "  question_tensor = torch.tensor(numerical_question).unsqueeze(0)\n",
        "\n",
        "  # send to model\n",
        "  output = model(question_tensor)\n",
        "\n",
        "  # convert logits to probs\n",
        "  probs = torch.nn.functional.softmax(output, dim=1)\n",
        "\n",
        "  # find index of max prob\n",
        "  value, index = torch.max(probs, dim=1)\n",
        "\n",
        "  if value < threshold:\n",
        "    print(\"I don't know\")\n",
        "\n",
        "  print(list(vocab.keys())[index])"
      ],
      "metadata": {
        "id": "BsVPXqSyDrIg"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(model , 'What is largest planet in our solar system')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63mtlIXFEvAd",
        "outputId": "5ccfa4c4-d9c1-40b1-a15e-4bb0125183f5"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jupiter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LuHkLWXWEzaQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}